# train.py

import torch
from torch.utils.data import DataLoader, TensorDataset
from teacher_model import TeacherNet
from student_model import StudentNet
from losses import total_loss
from augment_time_series import augment_batch
from prototype_selection import select_topk_prototypes

from data_preparation import prepare_data, create_patches
from analysis import load_data

# ðŸ“Œ Konfiguration
EPOCHS = 50
BATCH_SIZE = 32
LR = 1e-4
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
WINDOW_SIZE = 24
PATCH_SIZE = 6
STEP_SIZE = 1
EMBEDDING_DIM = 128
OUTPUT_DIM = 128
N_PROTOTYPES = 32

def main():
    # ðŸ”¹ 1. Daten laden
    df = load_data(['BTC/USDT', 'ETH/USDT'], interval="1h", since_days=100)
    X = prepare_data(df, window_size=WINDOW_SIZE, step_size=STEP_SIZE)  # (N, T, D)
    patches = create_patches(X, patch_size=PATCH_SIZE)  # (N, P, D, L)

    # In TensorDataset und Loader verpacken
    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32))
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    # ðŸ”¹ 2. Modelle
    teacher = TeacherNet(input_dim=X.shape[2], embedding_dim=EMBEDDING_DIM, output_dim=OUTPUT_DIM).to(DEVICE)
    student = StudentNet(input_dim=X.shape[2], embedding_dim=EMBEDDING_DIM, output_dim=OUTPUT_DIM,
                         n_prototypes=N_PROTOTYPES).to(DEVICE)

    # Nur Student trainieren
    optimizer = torch.optim.Adam(student.parameters(), lr=LR)

    teacher.eval()
    print(f"ðŸ“¦ Training auf {DEVICE} gestartet...")

    for epoch in range(EPOCHS):
        student.train()
        total_epoch_loss = 0

        for batch in loader:
            x = batch[0].to(DEVICE)  # (B, T, D)
            x_aug = augment_batch(x)

            # ðŸ”¹ Prototypen auswÃ¤hlen
            with torch.no_grad():
                feat_for_proto = student.embed_input(x)  # (B, emb_dim)
                topk_ids = select_topk_prototypes(feat_for_proto, student.prototypes, k=8)

            # ðŸ”¹ VorwÃ¤rtsdurchlÃ¤ufe
            with torch.no_grad():
                teacher_out = teacher(x)  # (B, output_dim)

            student_out = student(x, selected_proto_ids=topk_ids)
            student_aug_out = student(x_aug, selected_proto_ids=topk_ids)

            # ðŸ”¹ Loss berechnen (unsupervised â†’ alle Labels = 0)
            labels = torch.zeros(x.shape[0], device=DEVICE)
            loss, l_kd, l_ce = total_loss(student_out, teacher_out, student_aug_out, labels, lambda_ce=0.5)

            # ðŸ”¹ Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_epoch_loss += loss.item()

        avg_loss = total_epoch_loss / len(loader)
        print(f"ðŸ“‰ Epoch {epoch+1}/{EPOCHS} â€“ Loss: {avg_loss:.4f}")

    print("âœ… Training abgeschlossen.")

    # Optional: Speicher Modell
    torch.save(student.state_dict(), "student_model.pt")

if __name__ == "__main__":
    main()
